# ğŸ§Š Data Lake Architecture for Customer Analytics (AWS)

This project demonstrates how to build a **scalable Data Lake architecture** on AWS for storing, processing, and analyzing large-scale customer transaction data.

---

## ğŸš€ Features

- Amazon S3 for raw and processed data storage
- AWS Glue & PySpark for ETL transformation and schema enforcement
- AWS Lambda to automate and trigger data pipelines
- Amazon Redshift for warehousing and BI integration
- Designed for **50GB+ structured & semi-structured data**

---

## ğŸ—‚ï¸ Folder Structure

```
data-lake-customer-analytics/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ etl_pipeline_glue.py     # Glue ETL script
â”‚   â”œâ”€â”€ lambda_trigger.py        # Lambda trigger script
â”‚   â””â”€â”€ redshift_load.sql        # Load script for Redshift
â”œâ”€â”€ data/                        # Sample CSV data
â”œâ”€â”€ glue-job-config/            # Job parameters
â””â”€â”€ requirements.txt            # Python dependencies
```

---

## âš™ï¸ Technologies Used

- Python, PySpark
- Amazon S3, AWS Glue, AWS Lambda
- Amazon Redshift
- JSON, SQL

---

## ğŸ“ˆ Outcome

- Enabled real-time analytics using AWS services
- Reduced query latency by **35%**
- Processed and structured **50GB+** of raw customer data

---

## ğŸ“· Architecture Diagram

> *(You can add an image here â€” `architecture_diagram.png`)*

---

## ğŸ How to Run (Locally Simulated)

1. Place your sample transaction data in the `/data` folder
2. Modify the Glue script paths and run with `boto3` or AWS Console
3. Simulate Lambda trigger via `lambda_trigger.py`
4. Run `redshift_load.sql` to load data (if Redshift access configured)

---

## ğŸ“© Contact

**Sai Gayathri Makineni**  
Graduate Student, Computer Science â€“ University of North Texas  
ğŸ“§ [your email]  
ğŸŒ [LinkedIn link]
